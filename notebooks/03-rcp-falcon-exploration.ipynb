{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"tiiuae/falcon-rw-1b\"\n",
    "dataset_name=\"yelp_review_full\"\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset.  Just load the train split.  Different datasets have different splits, but\n",
    "# having a train split is common.\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want to use a small subset of the dataset for this example.\n",
    "# Note that dataset[:1000] would seem to work, but it doesn't.  In particular,\n",
    "# dataset[:1000] is not a dataset object, but a dictionary.  So, we use the\n",
    "# select method to get a dataset object.\n",
    "dataset = dataset.shuffle(42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2db33c8b4204daf8c64d0238c9a4626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This converts the dataset to a format that the model can understand.\n",
    "# In particlar, it takes the words and converts them to numbers/tokens.\n",
    "# Note, the pdding side is left since that is that the CausalLM model expects.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "# NOTE: the tokenizer.pad_token is a special token that is used to pad sequences to the same length.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# NOT TESTED: I think this gets a batch of samples as defined by the map function.\n",
    "# So, the longest refers to the longest sequence in the batch.\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding='longest', truncation=True, max_length=17)\n",
    "\n",
    "# NOTE: the map function does some fancy caching.  I.e., the first time you run it, it will\n",
    "# take a while.  But, the second time you run it, it will be much faster.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "# We don't need the labels anymore, so we remove them.\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"label\", \"text\"])\n",
    "# From https://huggingface.co/docs/datasets/v2.15.0/en/package_reference/main_classes#datasets.Dataset.set_format\n",
    "#     Set __getitem__ return format using this transform. The transform is applied on-the-fly on batches when __getitem__ is called. \n",
    "#     type (str, optional) â€” Either output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']. None means __getitem__ returns python objects (default).\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets, shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example text\n",
      "{'label': 4, 'text': \"I stalk this truck.  I've been to industrial parks where I pretend to be a tech worker standing in line, strip mall parking lots, and of course the farmer's market.  The bowls are so so absolutely divine.  The owner is super friendly and he makes each bowl by hand with an incredible amount of pride.  You gotta eat here guys!!!\"}\n",
      "example tokenized text\n",
      "{'input_ids': tensor([   40, 31297,   428,  7779,    13,   220,   314,  1053,   587,   284,\n",
      "         7593, 14860,   810,   314, 16614,   284,   307]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "example decoded tokenized text\n",
      "I stalk this truck.  I've been to\n"
     ]
    }
   ],
   "source": [
    "# A little sanity check.\n",
    "print('example text')\n",
    "print(dataset[0])\n",
    "print('example tokenized text')\n",
    "print(tokenized_datasets[0])\n",
    "print('example decoded tokenized text')\n",
    "print(tokenizer.decode(tokenized_datasets[0]['input_ids'][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spy(torch.nn.Module):\n",
    "        def __init__(self, model, debug=False):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "            self.debug = debug\n",
    "            self.inputs = []\n",
    "            self.outputs = []\n",
    "            self.last_size = 0\n",
    "\n",
    "        def forward(self, *args, **kwargs):\n",
    "            self.inputs.append(args)\n",
    "            output = self.model(*args, **kwargs)\n",
    "            self.outputs.append(output)\n",
    "            if self.debug:\n",
    "                print(f'args {args}')\n",
    "                print(f'kwargs {kwargs}')\n",
    "                print(f'output {output}')\n",
    "            return output\n",
    "\n",
    "        def print_last_input(self):\n",
    "            print(f'{self.last_size} {len(self.inputs)}')\n",
    "            for i in range(self.last_size, len(self.inputs)):\n",
    "                print(f'{i} {self.inputs[i][0].shape}')\n",
    "            self.last_size = len(self.inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FalconForCausalLM(\n",
      "  (transformer): FalconModel(\n",
      "    (word_embeddings): Embedding(50304, 2048)\n",
      "    (h): ModuleList(\n",
      "      (0-23): 24 x FalconDecoderLayer(\n",
      "        (self_attention): FalconAttention(\n",
      "          (query_key_value): FalconLinear(in_features=2048, out_features=6144, bias=True)\n",
      "          (dense): FalconLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): FalconMLP(\n",
      "          (dense_h_to_4h): FalconLinear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (dense_4h_to_h): FalconLinear(in_features=8192, out_features=2048, bias=True)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50304, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"tiiuae/falcon-rw-1b\":\n",
    "    my_spy = Spy(model.transformer.h[3].self_attention)\n",
    "    model.transformer.h[3].self_attention = my_spy\n",
    "elif model_name == \"bert-base-cased\":\n",
    "    my_spy = Spy(model.bert.encoder.layer[3])\n",
    "    model.bert.encoder.layer[3] = my_spy\n",
    "elif model_name == \"mistralai/Mistral-7B-v0.1\":\n",
    "    my_spy = Spy(model.model.layers[5])\n",
    "    model.model.layers[5] = my_spy\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"This is a review of a restaurant.  The food was\"\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = accelerator.prepare(model)\n",
    "# Note, the accelerator is cool, but only handles dataloaders.  So, for this example, we need to do it ourselves.\n",
    "input = input.to(accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rcpaffenroth/projects/inn_sequence/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n",
      "torch.Size([1, 112])\n",
      "0 100\n",
      "0 torch.Size([1, 12, 2048])\n",
      "1 torch.Size([1, 13, 2048])\n",
      "2 torch.Size([1, 14, 2048])\n",
      "3 torch.Size([1, 15, 2048])\n",
      "4 torch.Size([1, 16, 2048])\n",
      "5 torch.Size([1, 17, 2048])\n",
      "6 torch.Size([1, 18, 2048])\n",
      "7 torch.Size([1, 19, 2048])\n",
      "8 torch.Size([1, 20, 2048])\n",
      "9 torch.Size([1, 21, 2048])\n",
      "10 torch.Size([1, 22, 2048])\n",
      "11 torch.Size([1, 23, 2048])\n",
      "12 torch.Size([1, 24, 2048])\n",
      "13 torch.Size([1, 25, 2048])\n",
      "14 torch.Size([1, 26, 2048])\n",
      "15 torch.Size([1, 27, 2048])\n",
      "16 torch.Size([1, 28, 2048])\n",
      "17 torch.Size([1, 29, 2048])\n",
      "18 torch.Size([1, 30, 2048])\n",
      "19 torch.Size([1, 31, 2048])\n",
      "20 torch.Size([1, 32, 2048])\n",
      "21 torch.Size([1, 33, 2048])\n",
      "22 torch.Size([1, 34, 2048])\n",
      "23 torch.Size([1, 35, 2048])\n",
      "24 torch.Size([1, 36, 2048])\n",
      "25 torch.Size([1, 37, 2048])\n",
      "26 torch.Size([1, 38, 2048])\n",
      "27 torch.Size([1, 39, 2048])\n",
      "28 torch.Size([1, 40, 2048])\n",
      "29 torch.Size([1, 41, 2048])\n",
      "30 torch.Size([1, 42, 2048])\n",
      "31 torch.Size([1, 43, 2048])\n",
      "32 torch.Size([1, 44, 2048])\n",
      "33 torch.Size([1, 45, 2048])\n",
      "34 torch.Size([1, 46, 2048])\n",
      "35 torch.Size([1, 47, 2048])\n",
      "36 torch.Size([1, 48, 2048])\n",
      "37 torch.Size([1, 49, 2048])\n",
      "38 torch.Size([1, 50, 2048])\n",
      "39 torch.Size([1, 51, 2048])\n",
      "40 torch.Size([1, 52, 2048])\n",
      "41 torch.Size([1, 53, 2048])\n",
      "42 torch.Size([1, 54, 2048])\n",
      "43 torch.Size([1, 55, 2048])\n",
      "44 torch.Size([1, 56, 2048])\n",
      "45 torch.Size([1, 57, 2048])\n",
      "46 torch.Size([1, 58, 2048])\n",
      "47 torch.Size([1, 59, 2048])\n",
      "48 torch.Size([1, 60, 2048])\n",
      "49 torch.Size([1, 61, 2048])\n",
      "50 torch.Size([1, 62, 2048])\n",
      "51 torch.Size([1, 63, 2048])\n",
      "52 torch.Size([1, 64, 2048])\n",
      "53 torch.Size([1, 65, 2048])\n",
      "54 torch.Size([1, 66, 2048])\n",
      "55 torch.Size([1, 67, 2048])\n",
      "56 torch.Size([1, 68, 2048])\n",
      "57 torch.Size([1, 69, 2048])\n",
      "58 torch.Size([1, 70, 2048])\n",
      "59 torch.Size([1, 71, 2048])\n",
      "60 torch.Size([1, 72, 2048])\n",
      "61 torch.Size([1, 73, 2048])\n",
      "62 torch.Size([1, 74, 2048])\n",
      "63 torch.Size([1, 75, 2048])\n",
      "64 torch.Size([1, 76, 2048])\n",
      "65 torch.Size([1, 77, 2048])\n",
      "66 torch.Size([1, 78, 2048])\n",
      "67 torch.Size([1, 79, 2048])\n",
      "68 torch.Size([1, 80, 2048])\n",
      "69 torch.Size([1, 81, 2048])\n",
      "70 torch.Size([1, 82, 2048])\n",
      "71 torch.Size([1, 83, 2048])\n",
      "72 torch.Size([1, 84, 2048])\n",
      "73 torch.Size([1, 85, 2048])\n",
      "74 torch.Size([1, 86, 2048])\n",
      "75 torch.Size([1, 87, 2048])\n",
      "76 torch.Size([1, 88, 2048])\n",
      "77 torch.Size([1, 89, 2048])\n",
      "78 torch.Size([1, 90, 2048])\n",
      "79 torch.Size([1, 91, 2048])\n",
      "80 torch.Size([1, 92, 2048])\n",
      "81 torch.Size([1, 93, 2048])\n",
      "82 torch.Size([1, 94, 2048])\n",
      "83 torch.Size([1, 95, 2048])\n",
      "84 torch.Size([1, 96, 2048])\n",
      "85 torch.Size([1, 97, 2048])\n",
      "86 torch.Size([1, 98, 2048])\n",
      "87 torch.Size([1, 99, 2048])\n",
      "88 torch.Size([1, 100, 2048])\n",
      "89 torch.Size([1, 101, 2048])\n",
      "90 torch.Size([1, 102, 2048])\n",
      "91 torch.Size([1, 103, 2048])\n",
      "92 torch.Size([1, 104, 2048])\n",
      "93 torch.Size([1, 105, 2048])\n",
      "94 torch.Size([1, 106, 2048])\n",
      "95 torch.Size([1, 107, 2048])\n",
      "96 torch.Size([1, 108, 2048])\n",
      "97 torch.Size([1, 109, 2048])\n",
      "98 torch.Size([1, 110, 2048])\n",
      "99 torch.Size([1, 111, 2048])\n"
     ]
    }
   ],
   "source": [
    "# This is a simplier way to do it, but does not give high quality results.\n",
    "output = model.generate(input, max_new_tokens=100, use_cache=False, do_sample=False, top_k=50, top_p=0.95, pad_token_id=tokenizer.eos_token_id)\n",
    "print(input.shape)\n",
    "print(output.shape)\n",
    "my_spy.print_last_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt This is a review of a restaurant.  The food was\n",
      "generated_text This is a review of a restaurant.  The food was good, but the service was terrible. The waitress was rude and unhelpful. The food was good, but the service was terrible. The food was good, but the service was terrible. The food was good, but the service was terrible. The food was good, but the service was terrible. The food was good, but the service was terrible. The food was good, but the service was terrible. The food was good, but the service was terrible. The food was good, but\n"
     ]
    }
   ],
   "source": [
    "print(f'prompt {prompt}')\n",
    "print(f'generated_text {generated_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n",
      "torch.Size([1, 112])\n",
      "100 200\n",
      "100 torch.Size([1, 12, 2048])\n",
      "101 torch.Size([1, 13, 2048])\n",
      "102 torch.Size([1, 14, 2048])\n",
      "103 torch.Size([1, 15, 2048])\n",
      "104 torch.Size([1, 16, 2048])\n",
      "105 torch.Size([1, 17, 2048])\n",
      "106 torch.Size([1, 18, 2048])\n",
      "107 torch.Size([1, 19, 2048])\n",
      "108 torch.Size([1, 20, 2048])\n",
      "109 torch.Size([1, 21, 2048])\n",
      "110 torch.Size([1, 22, 2048])\n",
      "111 torch.Size([1, 23, 2048])\n",
      "112 torch.Size([1, 24, 2048])\n",
      "113 torch.Size([1, 25, 2048])\n",
      "114 torch.Size([1, 26, 2048])\n",
      "115 torch.Size([1, 27, 2048])\n",
      "116 torch.Size([1, 28, 2048])\n",
      "117 torch.Size([1, 29, 2048])\n",
      "118 torch.Size([1, 30, 2048])\n",
      "119 torch.Size([1, 31, 2048])\n",
      "120 torch.Size([1, 32, 2048])\n",
      "121 torch.Size([1, 33, 2048])\n",
      "122 torch.Size([1, 34, 2048])\n",
      "123 torch.Size([1, 35, 2048])\n",
      "124 torch.Size([1, 36, 2048])\n",
      "125 torch.Size([1, 37, 2048])\n",
      "126 torch.Size([1, 38, 2048])\n",
      "127 torch.Size([1, 39, 2048])\n",
      "128 torch.Size([1, 40, 2048])\n",
      "129 torch.Size([1, 41, 2048])\n",
      "130 torch.Size([1, 42, 2048])\n",
      "131 torch.Size([1, 43, 2048])\n",
      "132 torch.Size([1, 44, 2048])\n",
      "133 torch.Size([1, 45, 2048])\n",
      "134 torch.Size([1, 46, 2048])\n",
      "135 torch.Size([1, 47, 2048])\n",
      "136 torch.Size([1, 48, 2048])\n",
      "137 torch.Size([1, 49, 2048])\n",
      "138 torch.Size([1, 50, 2048])\n",
      "139 torch.Size([1, 51, 2048])\n",
      "140 torch.Size([1, 52, 2048])\n",
      "141 torch.Size([1, 53, 2048])\n",
      "142 torch.Size([1, 54, 2048])\n",
      "143 torch.Size([1, 55, 2048])\n",
      "144 torch.Size([1, 56, 2048])\n",
      "145 torch.Size([1, 57, 2048])\n",
      "146 torch.Size([1, 58, 2048])\n",
      "147 torch.Size([1, 59, 2048])\n",
      "148 torch.Size([1, 60, 2048])\n",
      "149 torch.Size([1, 61, 2048])\n",
      "150 torch.Size([1, 62, 2048])\n",
      "151 torch.Size([1, 63, 2048])\n",
      "152 torch.Size([1, 64, 2048])\n",
      "153 torch.Size([1, 65, 2048])\n",
      "154 torch.Size([1, 66, 2048])\n",
      "155 torch.Size([1, 67, 2048])\n",
      "156 torch.Size([1, 68, 2048])\n",
      "157 torch.Size([1, 69, 2048])\n",
      "158 torch.Size([1, 70, 2048])\n",
      "159 torch.Size([1, 71, 2048])\n",
      "160 torch.Size([1, 72, 2048])\n",
      "161 torch.Size([1, 73, 2048])\n",
      "162 torch.Size([1, 74, 2048])\n",
      "163 torch.Size([1, 75, 2048])\n",
      "164 torch.Size([1, 76, 2048])\n",
      "165 torch.Size([1, 77, 2048])\n",
      "166 torch.Size([1, 78, 2048])\n",
      "167 torch.Size([1, 79, 2048])\n",
      "168 torch.Size([1, 80, 2048])\n",
      "169 torch.Size([1, 81, 2048])\n",
      "170 torch.Size([1, 82, 2048])\n",
      "171 torch.Size([1, 83, 2048])\n",
      "172 torch.Size([1, 84, 2048])\n",
      "173 torch.Size([1, 85, 2048])\n",
      "174 torch.Size([1, 86, 2048])\n",
      "175 torch.Size([1, 87, 2048])\n",
      "176 torch.Size([1, 88, 2048])\n",
      "177 torch.Size([1, 89, 2048])\n",
      "178 torch.Size([1, 90, 2048])\n",
      "179 torch.Size([1, 91, 2048])\n",
      "180 torch.Size([1, 92, 2048])\n",
      "181 torch.Size([1, 93, 2048])\n",
      "182 torch.Size([1, 94, 2048])\n",
      "183 torch.Size([1, 95, 2048])\n",
      "184 torch.Size([1, 96, 2048])\n",
      "185 torch.Size([1, 97, 2048])\n",
      "186 torch.Size([1, 98, 2048])\n",
      "187 torch.Size([1, 99, 2048])\n",
      "188 torch.Size([1, 100, 2048])\n",
      "189 torch.Size([1, 101, 2048])\n",
      "190 torch.Size([1, 102, 2048])\n",
      "191 torch.Size([1, 103, 2048])\n",
      "192 torch.Size([1, 104, 2048])\n",
      "193 torch.Size([1, 105, 2048])\n",
      "194 torch.Size([1, 106, 2048])\n",
      "195 torch.Size([1, 107, 2048])\n",
      "196 torch.Size([1, 108, 2048])\n",
      "197 torch.Size([1, 109, 2048])\n",
      "198 torch.Size([1, 110, 2048])\n",
      "199 torch.Size([1, 111, 2048])\n"
     ]
    }
   ],
   "source": [
    "# This does some sampling that really seems to help, but I dont' understand it.\n",
    "# https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/text_generation#transformers.GenerationMixin.sample\n",
    "# Parameters that control the generation strategy used\n",
    "# do_sample (bool, optional, defaults to False) â€” Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "# num_beams (int, optional, defaults to 1) â€” Number of beams for beam search. 1 means no beam search.\n",
    "# num_beam_groups (int, optional, defaults to 1) â€” Number of groups to divide num_beams into in order to ensure diversity among different groups of beams. this paper for more details.\n",
    "# penalty_alpha (float, optional) â€” The values balance the model confidence and the degeneration penalty in contrastive search decoding.\n",
    "# use_cache (bool, optional, defaults to True) â€” Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.\n",
    "# Parameters for manipulation of the model output logits\n",
    "\n",
    "# temperature (float, optional, defaults to 1.0) â€” The value used to modulate the next token probabilities.\n",
    "# top_k (int, optional, defaults to 50) â€” The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "# top_p (float, optional, defaults to 1.0) â€” If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "# output = model.generate(input, max_new_tokens=100, do_sample=False, top_k=50, top_p=0.95, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Also, there is caching.  look at https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.max_new_tokens\n",
    "# use_cache â€” (bool, optional, defaults to True): Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.\n",
    "# The results are clearer if you set use_cache=False.  But, it is slower.\n",
    "output = model.generate(input, max_new_tokens=100, use_cache=False, do_sample=False, top_k=50, top_p=0.95, pad_token_id=tokenizer.eos_token_id)\n",
    "print(input.shape)\n",
    "print(output.shape)\n",
    "my_spy.print_last_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt This is a review of a restaurant.  The food was\n",
      "generated_text This is a review of a restaurant.  The food was good, but the service was terrible. The waitress was rude and unhelpful. The food was good, but the service was terrible. The food was good, but the service was terrible. The food was good, but the service was terrible. The food was good, but the service was terrible. The food was good, but the service was terrible. The food was good, but the service was terrible. The food was good, but the service was terrible. The food was good, but\n"
     ]
    }
   ],
   "source": [
    "print(f'prompt {prompt}')\n",
    "print(f'generated_text {generated_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.19 MiB is free. Process 497264 has 16.96 GiB memory in use. Including non-PyTorch memory, this process has 6.66 GiB memory in use. Of the allocated memory 5.22 GiB is allocated by PyTorch, and 137.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Note, the accelerator is cool, but only handles dataloaders.  So, for this example, we need to do it ourselves.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mto(accelerator\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m----> 5\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/projects/inn_sequence/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/inn_sequence/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1673\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1657\u001b[0m         input_ids,\n\u001b[1;32m   1658\u001b[0m         assistant_model\u001b[38;5;241m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1669\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1670\u001b[0m     )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1672\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/projects/inn_sequence/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2521\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2518\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2521\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2529\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/inn_sequence/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/inn_sequence/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/inn_sequence/.venv/lib/python3.10/site-packages/transformers/models/falcon/modeling_falcon.py:1260\u001b[0m, in \u001b[0;36mFalconForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1246\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[1;32m   1247\u001b[0m     input_ids,\n\u001b[1;32m   1248\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1256\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1257\u001b[0m )\n\u001b[1;32m   1258\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1260\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1262\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;66;03m# Shift so that tokens < n predict n\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/inn_sequence/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/inn_sequence/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/inn_sequence/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.19 MiB is free. Process 497264 has 16.96 GiB memory in use. Including non-PyTorch memory, this process has 6.66 GiB memory in use. Of the allocated memory 5.22 GiB is allocated by PyTorch, and 137.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "input = tokenized_datasets['input_ids'][:10]\n",
    "# Note, the accelerator is cool, but only handles dataloaders.  So, for this example, we need to do it ourselves.\n",
    "input = input.to(accelerator.device)\n",
    "\n",
    "output = model.generate(input, max_new_tokens=100, use_cache=False, do_sample=False, top_k=50, top_p=0.95, pad_token_id=tokenizer.eos_token_id)\n",
    "print(input.shape)\n",
    "print(output.shape)\n",
    "my_spy.print_last_input()\n",
    "\n",
    "for i in range(len(input)):\n",
    "    print('-----------------------------------------------')\n",
    "    print(f'input {tokenizer.decode(input[i])}')\n",
    "    print('...............................................')\n",
    "    print(f'output {tokenizer.decode(output[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 17])\n",
      "torch.Size([8, 117])\n",
      "300 400\n",
      "300 torch.Size([8, 17, 2048])\n",
      "301 torch.Size([8, 18, 2048])\n",
      "302 torch.Size([8, 19, 2048])\n",
      "303 torch.Size([8, 20, 2048])\n",
      "304 torch.Size([8, 21, 2048])\n",
      "305 torch.Size([8, 22, 2048])\n",
      "306 torch.Size([8, 23, 2048])\n",
      "307 torch.Size([8, 24, 2048])\n",
      "308 torch.Size([8, 25, 2048])\n",
      "309 torch.Size([8, 26, 2048])\n",
      "310 torch.Size([8, 27, 2048])\n",
      "311 torch.Size([8, 28, 2048])\n",
      "312 torch.Size([8, 29, 2048])\n",
      "313 torch.Size([8, 30, 2048])\n",
      "314 torch.Size([8, 31, 2048])\n",
      "315 torch.Size([8, 32, 2048])\n",
      "316 torch.Size([8, 33, 2048])\n",
      "317 torch.Size([8, 34, 2048])\n",
      "318 torch.Size([8, 35, 2048])\n",
      "319 torch.Size([8, 36, 2048])\n",
      "320 torch.Size([8, 37, 2048])\n",
      "321 torch.Size([8, 38, 2048])\n",
      "322 torch.Size([8, 39, 2048])\n",
      "323 torch.Size([8, 40, 2048])\n",
      "324 torch.Size([8, 41, 2048])\n",
      "325 torch.Size([8, 42, 2048])\n",
      "326 torch.Size([8, 43, 2048])\n",
      "327 torch.Size([8, 44, 2048])\n",
      "328 torch.Size([8, 45, 2048])\n",
      "329 torch.Size([8, 46, 2048])\n",
      "330 torch.Size([8, 47, 2048])\n",
      "331 torch.Size([8, 48, 2048])\n",
      "332 torch.Size([8, 49, 2048])\n",
      "333 torch.Size([8, 50, 2048])\n",
      "334 torch.Size([8, 51, 2048])\n",
      "335 torch.Size([8, 52, 2048])\n",
      "336 torch.Size([8, 53, 2048])\n",
      "337 torch.Size([8, 54, 2048])\n",
      "338 torch.Size([8, 55, 2048])\n",
      "339 torch.Size([8, 56, 2048])\n",
      "340 torch.Size([8, 57, 2048])\n",
      "341 torch.Size([8, 58, 2048])\n",
      "342 torch.Size([8, 59, 2048])\n",
      "343 torch.Size([8, 60, 2048])\n",
      "344 torch.Size([8, 61, 2048])\n",
      "345 torch.Size([8, 62, 2048])\n",
      "346 torch.Size([8, 63, 2048])\n",
      "347 torch.Size([8, 64, 2048])\n",
      "348 torch.Size([8, 65, 2048])\n",
      "349 torch.Size([8, 66, 2048])\n",
      "350 torch.Size([8, 67, 2048])\n",
      "351 torch.Size([8, 68, 2048])\n",
      "352 torch.Size([8, 69, 2048])\n",
      "353 torch.Size([8, 70, 2048])\n",
      "354 torch.Size([8, 71, 2048])\n",
      "355 torch.Size([8, 72, 2048])\n",
      "356 torch.Size([8, 73, 2048])\n",
      "357 torch.Size([8, 74, 2048])\n",
      "358 torch.Size([8, 75, 2048])\n",
      "359 torch.Size([8, 76, 2048])\n",
      "360 torch.Size([8, 77, 2048])\n",
      "361 torch.Size([8, 78, 2048])\n",
      "362 torch.Size([8, 79, 2048])\n",
      "363 torch.Size([8, 80, 2048])\n",
      "364 torch.Size([8, 81, 2048])\n",
      "365 torch.Size([8, 82, 2048])\n",
      "366 torch.Size([8, 83, 2048])\n",
      "367 torch.Size([8, 84, 2048])\n",
      "368 torch.Size([8, 85, 2048])\n",
      "369 torch.Size([8, 86, 2048])\n",
      "370 torch.Size([8, 87, 2048])\n",
      "371 torch.Size([8, 88, 2048])\n",
      "372 torch.Size([8, 89, 2048])\n",
      "373 torch.Size([8, 90, 2048])\n",
      "374 torch.Size([8, 91, 2048])\n",
      "375 torch.Size([8, 92, 2048])\n",
      "376 torch.Size([8, 93, 2048])\n",
      "377 torch.Size([8, 94, 2048])\n",
      "378 torch.Size([8, 95, 2048])\n",
      "379 torch.Size([8, 96, 2048])\n",
      "380 torch.Size([8, 97, 2048])\n",
      "381 torch.Size([8, 98, 2048])\n",
      "382 torch.Size([8, 99, 2048])\n",
      "383 torch.Size([8, 100, 2048])\n",
      "384 torch.Size([8, 101, 2048])\n",
      "385 torch.Size([8, 102, 2048])\n",
      "386 torch.Size([8, 103, 2048])\n",
      "387 torch.Size([8, 104, 2048])\n",
      "388 torch.Size([8, 105, 2048])\n",
      "389 torch.Size([8, 106, 2048])\n",
      "390 torch.Size([8, 107, 2048])\n",
      "391 torch.Size([8, 108, 2048])\n",
      "392 torch.Size([8, 109, 2048])\n",
      "393 torch.Size([8, 110, 2048])\n",
      "394 torch.Size([8, 111, 2048])\n",
      "395 torch.Size([8, 112, 2048])\n",
      "396 torch.Size([8, 113, 2048])\n",
      "397 torch.Size([8, 114, 2048])\n",
      "398 torch.Size([8, 115, 2048])\n",
      "399 torch.Size([8, 116, 2048])\n",
      "-----------------------------------------------\n",
      "input What better way to celebrate my 29th birthday than to try to live it up like\n",
      "...............................................\n",
      "output What better way to celebrate my 29th birthday than to try to live it up like a rock star?\n",
      "Iâ€™m not sure if Iâ€™m a rock star or not, but Iâ€™m certainly a rock star in my own mind.\n",
      "Iâ€™m a rock star in my own mind because Iâ€™m a rock star in my own head.\n",
      "Iâ€™m a rock star in my own mind because Iâ€™m a rock star in my own heart.\n",
      "Iâ€™m a rock star in my own mind because I\n",
      "-----------------------------------------------\n",
      "input Outatanding.   Lump crab cales were all crab.  Loves\n",
      "...............................................\n",
      "output Outatanding.   Lump crab cales were all crab.  Loves the place. \n",
      "The food was great. \n",
      "The food was great. \n",
      "The food was great. \n",
      "The food was great. \n",
      "The food was great. \n",
      "The food was great. \n",
      "The food was great. \n",
      "The food was great. \n",
      "The food was great. \n",
      "The food was great. \n",
      "The food was great. \n",
      "The food was great. \n",
      "The food was great. \n",
      "The food was great\n",
      "-----------------------------------------------\n",
      "input My experience going here has always been excellent. The people are nice. I've never\n",
      "...............................................\n",
      "output My experience going here has always been excellent. The people are nice. I've never had a problem. I've been here for a year and a half. I've been here for a year and a half. I've been here for a year and a half. I've been here for a year and a half. I've been here for a year and a half. I've been here for a year and a half. I've been here for a year and a half. I've been here for a year and a half. I've been here for a year and\n",
      "-----------------------------------------------\n",
      "input Wow, an AM/PM that doesn't slam you with fees for using a debit\n",
      "...............................................\n",
      "output Wow, an AM/PM that doesn't slam you with fees for using a debit card.\n",
      "I'm not sure what the point of this is.\n",
      "I'm not sure what the point of this is.\n",
      "I'm not sure what the point of this is.\n",
      "I'm not sure what the point of this is.\n",
      "I'm not sure what the point of this is.\n",
      "I'm not sure what the point of this is.\n",
      "I'm not sure what the point of this is.\n",
      "I'm not sure what the point of this is.\n",
      "I\n",
      "-----------------------------------------------\n",
      "input Having read the reviews and checked out the menu and decor in person before deciding on it\n",
      "...............................................\n",
      "output Having read the reviews and checked out the menu and decor in person before deciding on it, I was really looking forward to my first visit to the new restaurant in the old St. Jamesâ€™s Hotel.\n",
      "The restaurant is located on the ground floor of the hotel, and is a very pleasant space. The decor is very contemporary, with a lot of wood and glass, and the lighting is very flattering. The staff were very friendly and welcoming, and the food was excellent.\n",
      "The menu is a mix of classic dishes and new creations, and the food was very well presented\n",
      "-----------------------------------------------\n",
      "input Wanted to find a place to buy tortillas since I moved here. When I\n",
      "...............................................\n",
      "output Wanted to find a place to buy tortillas since I moved here. When I found this place, I was so happy. The tortillas are fresh and delicious. I love the place.\n",
      "I love this place. I have been going here for years. The tortillas are fresh and delicious. I love the place.\n",
      "I love this place. I have been going here for years. The tortillas are fresh and delicious. I love the place.\n",
      "I love this place. I have been going here for years. The tortillas are fresh and delicious. I love\n",
      "-----------------------------------------------\n",
      "input Overrated.\\n\\nIt's a good burger. Service and quality are\n",
      "...............................................\n",
      "output Overrated.\\n\\nIt's a good burger. Service and quality are good.\\n\\nI've been to the one in the mall and the one in the city. The one in the mall is better.\\n\\nI've been to the one in the mall and the one in the city. The one in the mall is better.\\n\\nI've been to the one in the mall and the one in the city. The one in the mall is better.\\n\\nI've been to the one in the mall and\n",
      "-----------------------------------------------\n",
      "input Ice Pan is an interesting take on an already saturated chilled dessert market. The iced\n",
      "...............................................\n",
      "output Ice Pan is an interesting take on an already saturated chilled dessert market. The iced coffee ice cream is a unique twist on a classic, and the ice cream is made with real coffee beans.\n",
      "The ice cream is made with real coffee beans, and the coffee is sourced from a local roaster. The ice cream is made with real coffee beans, and the coffee is sourced from a local roaster.\n",
      "The ice cream is made with real coffee beans, and the coffee is sourced from a local roaster. The ice cream is made with real coffee beans, and the coffee\n"
     ]
    }
   ],
   "source": [
    "# Ok, now we do the same thing, but with a dataloader.\n",
    "model, train_dataloader = accelerator.prepare(model, train_dataloader)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "input = batch['input_ids']\n",
    "\n",
    "output = model.generate(input, max_new_tokens=100, use_cache=False, do_sample=False, top_k=50, top_p=0.95, pad_token_id=tokenizer.eos_token_id)\n",
    "print(input.shape)\n",
    "print(output.shape)\n",
    "my_spy.print_last_input()\n",
    "\n",
    "for i in range(len(input)):\n",
    "    print('-----------------------------------------------')\n",
    "    print(f'input {tokenizer.decode(input[i])}')\n",
    "    print('...............................................')\n",
    "    print(f'output {tokenizer.decode(output[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(my_spy.inputs)):\n",
    "    tmp_len = len(my_spy.inputs[i])\n",
    "    if tmp_len != 1:\n",
    "        print(f'{i} {tmp_len}')\n",
    "# Every tuple if of length 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 12, 2048])\n",
      "1 torch.Size([1, 13, 2048])\n",
      "2 torch.Size([1, 14, 2048])\n",
      "3 torch.Size([1, 15, 2048])\n",
      "4 torch.Size([1, 16, 2048])\n",
      "5 torch.Size([1, 17, 2048])\n",
      "6 torch.Size([1, 18, 2048])\n",
      "7 torch.Size([1, 19, 2048])\n",
      "8 torch.Size([1, 20, 2048])\n",
      "9 torch.Size([1, 21, 2048])\n",
      "10 torch.Size([1, 22, 2048])\n",
      "11 torch.Size([1, 23, 2048])\n",
      "12 torch.Size([1, 24, 2048])\n",
      "13 torch.Size([1, 25, 2048])\n",
      "14 torch.Size([1, 26, 2048])\n",
      "15 torch.Size([1, 27, 2048])\n",
      "16 torch.Size([1, 28, 2048])\n",
      "17 torch.Size([1, 29, 2048])\n",
      "18 torch.Size([1, 30, 2048])\n",
      "19 torch.Size([1, 31, 2048])\n",
      "20 torch.Size([1, 32, 2048])\n",
      "21 torch.Size([1, 33, 2048])\n",
      "22 torch.Size([1, 34, 2048])\n",
      "23 torch.Size([1, 35, 2048])\n",
      "24 torch.Size([1, 36, 2048])\n",
      "25 torch.Size([1, 37, 2048])\n",
      "26 torch.Size([1, 38, 2048])\n",
      "27 torch.Size([1, 39, 2048])\n",
      "28 torch.Size([1, 40, 2048])\n",
      "29 torch.Size([1, 41, 2048])\n",
      "30 torch.Size([1, 42, 2048])\n",
      "31 torch.Size([1, 43, 2048])\n",
      "32 torch.Size([1, 44, 2048])\n",
      "33 torch.Size([1, 45, 2048])\n",
      "34 torch.Size([1, 46, 2048])\n",
      "35 torch.Size([1, 47, 2048])\n",
      "36 torch.Size([1, 48, 2048])\n",
      "37 torch.Size([1, 49, 2048])\n",
      "38 torch.Size([1, 50, 2048])\n",
      "39 torch.Size([1, 51, 2048])\n",
      "40 torch.Size([1, 52, 2048])\n",
      "41 torch.Size([1, 53, 2048])\n",
      "42 torch.Size([1, 54, 2048])\n",
      "43 torch.Size([1, 55, 2048])\n",
      "44 torch.Size([1, 56, 2048])\n",
      "45 torch.Size([1, 57, 2048])\n",
      "46 torch.Size([1, 58, 2048])\n",
      "47 torch.Size([1, 59, 2048])\n",
      "48 torch.Size([1, 60, 2048])\n",
      "49 torch.Size([1, 61, 2048])\n",
      "50 torch.Size([1, 62, 2048])\n",
      "51 torch.Size([1, 63, 2048])\n",
      "52 torch.Size([1, 64, 2048])\n",
      "53 torch.Size([1, 65, 2048])\n",
      "54 torch.Size([1, 66, 2048])\n",
      "55 torch.Size([1, 67, 2048])\n",
      "56 torch.Size([1, 68, 2048])\n",
      "57 torch.Size([1, 69, 2048])\n",
      "58 torch.Size([1, 70, 2048])\n",
      "59 torch.Size([1, 71, 2048])\n",
      "60 torch.Size([1, 72, 2048])\n",
      "61 torch.Size([1, 73, 2048])\n",
      "62 torch.Size([1, 74, 2048])\n",
      "63 torch.Size([1, 75, 2048])\n",
      "64 torch.Size([1, 76, 2048])\n",
      "65 torch.Size([1, 77, 2048])\n",
      "66 torch.Size([1, 78, 2048])\n",
      "67 torch.Size([1, 79, 2048])\n",
      "68 torch.Size([1, 80, 2048])\n",
      "69 torch.Size([1, 81, 2048])\n",
      "70 torch.Size([1, 82, 2048])\n",
      "71 torch.Size([1, 83, 2048])\n",
      "72 torch.Size([1, 84, 2048])\n",
      "73 torch.Size([1, 85, 2048])\n",
      "74 torch.Size([1, 86, 2048])\n",
      "75 torch.Size([1, 87, 2048])\n",
      "76 torch.Size([1, 88, 2048])\n",
      "77 torch.Size([1, 89, 2048])\n",
      "78 torch.Size([1, 90, 2048])\n",
      "79 torch.Size([1, 91, 2048])\n",
      "80 torch.Size([1, 92, 2048])\n",
      "81 torch.Size([1, 93, 2048])\n",
      "82 torch.Size([1, 94, 2048])\n",
      "83 torch.Size([1, 95, 2048])\n",
      "84 torch.Size([1, 96, 2048])\n",
      "85 torch.Size([1, 97, 2048])\n",
      "86 torch.Size([1, 98, 2048])\n",
      "87 torch.Size([1, 99, 2048])\n",
      "88 torch.Size([1, 100, 2048])\n",
      "89 torch.Size([1, 101, 2048])\n",
      "90 torch.Size([1, 102, 2048])\n",
      "91 torch.Size([1, 103, 2048])\n",
      "92 torch.Size([1, 104, 2048])\n",
      "93 torch.Size([1, 105, 2048])\n",
      "94 torch.Size([1, 106, 2048])\n",
      "95 torch.Size([1, 107, 2048])\n",
      "96 torch.Size([1, 108, 2048])\n",
      "97 torch.Size([1, 109, 2048])\n",
      "98 torch.Size([1, 110, 2048])\n",
      "99 torch.Size([1, 111, 2048])\n",
      "100 torch.Size([1, 12, 2048])\n",
      "101 torch.Size([1, 13, 2048])\n",
      "102 torch.Size([1, 14, 2048])\n",
      "103 torch.Size([1, 15, 2048])\n",
      "104 torch.Size([1, 16, 2048])\n",
      "105 torch.Size([1, 17, 2048])\n",
      "106 torch.Size([1, 18, 2048])\n",
      "107 torch.Size([1, 19, 2048])\n",
      "108 torch.Size([1, 20, 2048])\n",
      "109 torch.Size([1, 21, 2048])\n",
      "110 torch.Size([1, 22, 2048])\n",
      "111 torch.Size([1, 23, 2048])\n",
      "112 torch.Size([1, 24, 2048])\n",
      "113 torch.Size([1, 25, 2048])\n",
      "114 torch.Size([1, 26, 2048])\n",
      "115 torch.Size([1, 27, 2048])\n",
      "116 torch.Size([1, 28, 2048])\n",
      "117 torch.Size([1, 29, 2048])\n",
      "118 torch.Size([1, 30, 2048])\n",
      "119 torch.Size([1, 31, 2048])\n",
      "120 torch.Size([1, 32, 2048])\n",
      "121 torch.Size([1, 33, 2048])\n",
      "122 torch.Size([1, 34, 2048])\n",
      "123 torch.Size([1, 35, 2048])\n",
      "124 torch.Size([1, 36, 2048])\n",
      "125 torch.Size([1, 37, 2048])\n",
      "126 torch.Size([1, 38, 2048])\n",
      "127 torch.Size([1, 39, 2048])\n",
      "128 torch.Size([1, 40, 2048])\n",
      "129 torch.Size([1, 41, 2048])\n",
      "130 torch.Size([1, 42, 2048])\n",
      "131 torch.Size([1, 43, 2048])\n",
      "132 torch.Size([1, 44, 2048])\n",
      "133 torch.Size([1, 45, 2048])\n",
      "134 torch.Size([1, 46, 2048])\n",
      "135 torch.Size([1, 47, 2048])\n",
      "136 torch.Size([1, 48, 2048])\n",
      "137 torch.Size([1, 49, 2048])\n",
      "138 torch.Size([1, 50, 2048])\n",
      "139 torch.Size([1, 51, 2048])\n",
      "140 torch.Size([1, 52, 2048])\n",
      "141 torch.Size([1, 53, 2048])\n",
      "142 torch.Size([1, 54, 2048])\n",
      "143 torch.Size([1, 55, 2048])\n",
      "144 torch.Size([1, 56, 2048])\n",
      "145 torch.Size([1, 57, 2048])\n",
      "146 torch.Size([1, 58, 2048])\n",
      "147 torch.Size([1, 59, 2048])\n",
      "148 torch.Size([1, 60, 2048])\n",
      "149 torch.Size([1, 61, 2048])\n",
      "150 torch.Size([1, 62, 2048])\n",
      "151 torch.Size([1, 63, 2048])\n",
      "152 torch.Size([1, 64, 2048])\n",
      "153 torch.Size([1, 65, 2048])\n",
      "154 torch.Size([1, 66, 2048])\n",
      "155 torch.Size([1, 67, 2048])\n",
      "156 torch.Size([1, 68, 2048])\n",
      "157 torch.Size([1, 69, 2048])\n",
      "158 torch.Size([1, 70, 2048])\n",
      "159 torch.Size([1, 71, 2048])\n",
      "160 torch.Size([1, 72, 2048])\n",
      "161 torch.Size([1, 73, 2048])\n",
      "162 torch.Size([1, 74, 2048])\n",
      "163 torch.Size([1, 75, 2048])\n",
      "164 torch.Size([1, 76, 2048])\n",
      "165 torch.Size([1, 77, 2048])\n",
      "166 torch.Size([1, 78, 2048])\n",
      "167 torch.Size([1, 79, 2048])\n",
      "168 torch.Size([1, 80, 2048])\n",
      "169 torch.Size([1, 81, 2048])\n",
      "170 torch.Size([1, 82, 2048])\n",
      "171 torch.Size([1, 83, 2048])\n",
      "172 torch.Size([1, 84, 2048])\n",
      "173 torch.Size([1, 85, 2048])\n",
      "174 torch.Size([1, 86, 2048])\n",
      "175 torch.Size([1, 87, 2048])\n",
      "176 torch.Size([1, 88, 2048])\n",
      "177 torch.Size([1, 89, 2048])\n",
      "178 torch.Size([1, 90, 2048])\n",
      "179 torch.Size([1, 91, 2048])\n",
      "180 torch.Size([1, 92, 2048])\n",
      "181 torch.Size([1, 93, 2048])\n",
      "182 torch.Size([1, 94, 2048])\n",
      "183 torch.Size([1, 95, 2048])\n",
      "184 torch.Size([1, 96, 2048])\n",
      "185 torch.Size([1, 97, 2048])\n",
      "186 torch.Size([1, 98, 2048])\n",
      "187 torch.Size([1, 99, 2048])\n",
      "188 torch.Size([1, 100, 2048])\n",
      "189 torch.Size([1, 101, 2048])\n",
      "190 torch.Size([1, 102, 2048])\n",
      "191 torch.Size([1, 103, 2048])\n",
      "192 torch.Size([1, 104, 2048])\n",
      "193 torch.Size([1, 105, 2048])\n",
      "194 torch.Size([1, 106, 2048])\n",
      "195 torch.Size([1, 107, 2048])\n",
      "196 torch.Size([1, 108, 2048])\n",
      "197 torch.Size([1, 109, 2048])\n",
      "198 torch.Size([1, 110, 2048])\n",
      "199 torch.Size([1, 111, 2048])\n",
      "200 torch.Size([10, 17, 2048])\n",
      "201 torch.Size([10, 18, 2048])\n",
      "202 torch.Size([10, 19, 2048])\n",
      "203 torch.Size([10, 20, 2048])\n",
      "204 torch.Size([10, 21, 2048])\n",
      "205 torch.Size([10, 22, 2048])\n",
      "206 torch.Size([10, 23, 2048])\n",
      "207 torch.Size([10, 24, 2048])\n",
      "208 torch.Size([10, 25, 2048])\n",
      "209 torch.Size([10, 26, 2048])\n",
      "210 torch.Size([10, 27, 2048])\n",
      "211 torch.Size([10, 28, 2048])\n",
      "212 torch.Size([10, 29, 2048])\n",
      "213 torch.Size([10, 30, 2048])\n",
      "214 torch.Size([10, 31, 2048])\n",
      "215 torch.Size([10, 32, 2048])\n",
      "216 torch.Size([10, 33, 2048])\n",
      "217 torch.Size([10, 34, 2048])\n",
      "218 torch.Size([10, 35, 2048])\n",
      "219 torch.Size([10, 36, 2048])\n",
      "220 torch.Size([10, 37, 2048])\n",
      "221 torch.Size([10, 38, 2048])\n",
      "222 torch.Size([10, 39, 2048])\n",
      "223 torch.Size([10, 40, 2048])\n",
      "224 torch.Size([10, 41, 2048])\n",
      "225 torch.Size([10, 42, 2048])\n",
      "226 torch.Size([10, 43, 2048])\n",
      "227 torch.Size([10, 44, 2048])\n",
      "228 torch.Size([10, 45, 2048])\n",
      "229 torch.Size([10, 46, 2048])\n",
      "230 torch.Size([10, 47, 2048])\n",
      "231 torch.Size([10, 48, 2048])\n",
      "232 torch.Size([10, 49, 2048])\n",
      "233 torch.Size([10, 50, 2048])\n",
      "234 torch.Size([10, 51, 2048])\n",
      "235 torch.Size([10, 52, 2048])\n",
      "236 torch.Size([10, 53, 2048])\n",
      "237 torch.Size([10, 54, 2048])\n",
      "238 torch.Size([10, 55, 2048])\n",
      "239 torch.Size([10, 56, 2048])\n",
      "240 torch.Size([10, 57, 2048])\n",
      "241 torch.Size([10, 58, 2048])\n",
      "242 torch.Size([10, 59, 2048])\n",
      "243 torch.Size([10, 60, 2048])\n",
      "244 torch.Size([10, 61, 2048])\n",
      "245 torch.Size([10, 62, 2048])\n",
      "246 torch.Size([10, 63, 2048])\n",
      "247 torch.Size([10, 64, 2048])\n",
      "248 torch.Size([10, 65, 2048])\n",
      "249 torch.Size([10, 66, 2048])\n",
      "250 torch.Size([10, 67, 2048])\n",
      "251 torch.Size([10, 68, 2048])\n",
      "252 torch.Size([10, 69, 2048])\n",
      "253 torch.Size([10, 70, 2048])\n",
      "254 torch.Size([10, 71, 2048])\n",
      "255 torch.Size([10, 72, 2048])\n",
      "256 torch.Size([10, 73, 2048])\n",
      "257 torch.Size([10, 74, 2048])\n",
      "258 torch.Size([10, 75, 2048])\n",
      "259 torch.Size([10, 76, 2048])\n",
      "260 torch.Size([10, 77, 2048])\n",
      "261 torch.Size([10, 78, 2048])\n",
      "262 torch.Size([10, 79, 2048])\n",
      "263 torch.Size([10, 80, 2048])\n",
      "264 torch.Size([10, 81, 2048])\n",
      "265 torch.Size([10, 82, 2048])\n",
      "266 torch.Size([10, 83, 2048])\n",
      "267 torch.Size([10, 84, 2048])\n",
      "268 torch.Size([10, 85, 2048])\n",
      "269 torch.Size([10, 86, 2048])\n",
      "270 torch.Size([10, 87, 2048])\n",
      "271 torch.Size([10, 88, 2048])\n",
      "272 torch.Size([10, 89, 2048])\n",
      "273 torch.Size([10, 90, 2048])\n",
      "274 torch.Size([10, 91, 2048])\n",
      "275 torch.Size([10, 92, 2048])\n",
      "276 torch.Size([10, 93, 2048])\n",
      "277 torch.Size([10, 94, 2048])\n",
      "278 torch.Size([10, 95, 2048])\n",
      "279 torch.Size([10, 96, 2048])\n",
      "280 torch.Size([10, 97, 2048])\n",
      "281 torch.Size([10, 98, 2048])\n",
      "282 torch.Size([10, 99, 2048])\n",
      "283 torch.Size([10, 100, 2048])\n",
      "284 torch.Size([10, 101, 2048])\n",
      "285 torch.Size([10, 102, 2048])\n",
      "286 torch.Size([10, 103, 2048])\n",
      "287 torch.Size([10, 104, 2048])\n",
      "288 torch.Size([10, 105, 2048])\n",
      "289 torch.Size([10, 106, 2048])\n",
      "290 torch.Size([10, 107, 2048])\n",
      "291 torch.Size([10, 108, 2048])\n",
      "292 torch.Size([10, 109, 2048])\n",
      "293 torch.Size([10, 110, 2048])\n",
      "294 torch.Size([10, 111, 2048])\n",
      "295 torch.Size([10, 112, 2048])\n",
      "296 torch.Size([10, 113, 2048])\n",
      "297 torch.Size([10, 114, 2048])\n",
      "298 torch.Size([10, 115, 2048])\n",
      "299 torch.Size([10, 116, 2048])\n",
      "300 torch.Size([8, 17, 2048])\n",
      "301 torch.Size([8, 18, 2048])\n",
      "302 torch.Size([8, 19, 2048])\n",
      "303 torch.Size([8, 20, 2048])\n",
      "304 torch.Size([8, 21, 2048])\n",
      "305 torch.Size([8, 22, 2048])\n",
      "306 torch.Size([8, 23, 2048])\n",
      "307 torch.Size([8, 24, 2048])\n",
      "308 torch.Size([8, 25, 2048])\n",
      "309 torch.Size([8, 26, 2048])\n",
      "310 torch.Size([8, 27, 2048])\n",
      "311 torch.Size([8, 28, 2048])\n",
      "312 torch.Size([8, 29, 2048])\n",
      "313 torch.Size([8, 30, 2048])\n",
      "314 torch.Size([8, 31, 2048])\n",
      "315 torch.Size([8, 32, 2048])\n",
      "316 torch.Size([8, 33, 2048])\n",
      "317 torch.Size([8, 34, 2048])\n",
      "318 torch.Size([8, 35, 2048])\n",
      "319 torch.Size([8, 36, 2048])\n",
      "320 torch.Size([8, 37, 2048])\n",
      "321 torch.Size([8, 38, 2048])\n",
      "322 torch.Size([8, 39, 2048])\n",
      "323 torch.Size([8, 40, 2048])\n",
      "324 torch.Size([8, 41, 2048])\n",
      "325 torch.Size([8, 42, 2048])\n",
      "326 torch.Size([8, 43, 2048])\n",
      "327 torch.Size([8, 44, 2048])\n",
      "328 torch.Size([8, 45, 2048])\n",
      "329 torch.Size([8, 46, 2048])\n",
      "330 torch.Size([8, 47, 2048])\n",
      "331 torch.Size([8, 48, 2048])\n",
      "332 torch.Size([8, 49, 2048])\n",
      "333 torch.Size([8, 50, 2048])\n",
      "334 torch.Size([8, 51, 2048])\n",
      "335 torch.Size([8, 52, 2048])\n",
      "336 torch.Size([8, 53, 2048])\n",
      "337 torch.Size([8, 54, 2048])\n",
      "338 torch.Size([8, 55, 2048])\n",
      "339 torch.Size([8, 56, 2048])\n",
      "340 torch.Size([8, 57, 2048])\n",
      "341 torch.Size([8, 58, 2048])\n",
      "342 torch.Size([8, 59, 2048])\n",
      "343 torch.Size([8, 60, 2048])\n",
      "344 torch.Size([8, 61, 2048])\n",
      "345 torch.Size([8, 62, 2048])\n",
      "346 torch.Size([8, 63, 2048])\n",
      "347 torch.Size([8, 64, 2048])\n",
      "348 torch.Size([8, 65, 2048])\n",
      "349 torch.Size([8, 66, 2048])\n",
      "350 torch.Size([8, 67, 2048])\n",
      "351 torch.Size([8, 68, 2048])\n",
      "352 torch.Size([8, 69, 2048])\n",
      "353 torch.Size([8, 70, 2048])\n",
      "354 torch.Size([8, 71, 2048])\n",
      "355 torch.Size([8, 72, 2048])\n",
      "356 torch.Size([8, 73, 2048])\n",
      "357 torch.Size([8, 74, 2048])\n",
      "358 torch.Size([8, 75, 2048])\n",
      "359 torch.Size([8, 76, 2048])\n",
      "360 torch.Size([8, 77, 2048])\n",
      "361 torch.Size([8, 78, 2048])\n",
      "362 torch.Size([8, 79, 2048])\n",
      "363 torch.Size([8, 80, 2048])\n",
      "364 torch.Size([8, 81, 2048])\n",
      "365 torch.Size([8, 82, 2048])\n",
      "366 torch.Size([8, 83, 2048])\n",
      "367 torch.Size([8, 84, 2048])\n",
      "368 torch.Size([8, 85, 2048])\n",
      "369 torch.Size([8, 86, 2048])\n",
      "370 torch.Size([8, 87, 2048])\n",
      "371 torch.Size([8, 88, 2048])\n",
      "372 torch.Size([8, 89, 2048])\n",
      "373 torch.Size([8, 90, 2048])\n",
      "374 torch.Size([8, 91, 2048])\n",
      "375 torch.Size([8, 92, 2048])\n",
      "376 torch.Size([8, 93, 2048])\n",
      "377 torch.Size([8, 94, 2048])\n",
      "378 torch.Size([8, 95, 2048])\n",
      "379 torch.Size([8, 96, 2048])\n",
      "380 torch.Size([8, 97, 2048])\n",
      "381 torch.Size([8, 98, 2048])\n",
      "382 torch.Size([8, 99, 2048])\n",
      "383 torch.Size([8, 100, 2048])\n",
      "384 torch.Size([8, 101, 2048])\n",
      "385 torch.Size([8, 102, 2048])\n",
      "386 torch.Size([8, 103, 2048])\n",
      "387 torch.Size([8, 104, 2048])\n",
      "388 torch.Size([8, 105, 2048])\n",
      "389 torch.Size([8, 106, 2048])\n",
      "390 torch.Size([8, 107, 2048])\n",
      "391 torch.Size([8, 108, 2048])\n",
      "392 torch.Size([8, 109, 2048])\n",
      "393 torch.Size([8, 110, 2048])\n",
      "394 torch.Size([8, 111, 2048])\n",
      "395 torch.Size([8, 112, 2048])\n",
      "396 torch.Size([8, 113, 2048])\n",
      "397 torch.Size([8, 114, 2048])\n",
      "398 torch.Size([8, 115, 2048])\n",
      "399 torch.Size([8, 116, 2048])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(my_spy.inputs)):\n",
    "    tmp_shape = my_spy.inputs[i][0].shape\n",
    "    print(f'{i} {tmp_shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = []\n",
    "for i in range(len(my_spy.inputs)):\n",
    "   sizes.append(my_spy.inputs[i][0].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc91e989420>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FalconConfig {\n",
       "  \"_name_or_path\": \"tiiuae/falcon-rw-1b\",\n",
       "  \"alibi\": true,\n",
       "  \"apply_residual_connection_post_layernorm\": false,\n",
       "  \"architectures\": [\n",
       "    \"FalconForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"auto_map\": {\n",
       "    \"AutoConfig\": \"tiiuae/falcon-rw-1b--configuration_falcon.FalconConfig\",\n",
       "    \"AutoModel\": \"tiiuae/falcon-rw-1b--modeling_falcon.FalconModel\",\n",
       "    \"AutoModelForCausalLM\": \"tiiuae/falcon-rw-1b--modeling_falcon.FalconForCausalLM\",\n",
       "    \"AutoModelForQuestionAnswering\": \"tiiuae/falcon-rw-1b--modeling_falcon.FalconForQuestionAnswering\",\n",
       "    \"AutoModelForSequenceClassification\": \"tiiuae/falcon-rw-1b--modeling_falcon.FalconForSequenceClassification\",\n",
       "    \"AutoModelForTokenClassification\": \"tiiuae/falcon-rw-1b--modeling_falcon.FalconForTokenClassification\"\n",
       "  },\n",
       "  \"bias\": true,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_dropout\": 0.0,\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"falcon\",\n",
       "  \"multi_query\": false,\n",
       "  \"new_decoder_architecture\": false,\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"num_kv_heads\": 32,\n",
       "  \"parallel_attn\": false,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.35.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50304\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FalconForCausalLM(\n",
       "  (transformer): FalconModel(\n",
       "    (word_embeddings): Embedding(50304, 2048)\n",
       "    (h): ModuleList(\n",
       "      (0-2): 3 x FalconDecoderLayer(\n",
       "        (self_attention): FalconAttention(\n",
       "          (query_key_value): FalconLinear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): FalconLinear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): FalconMLP(\n",
       "          (dense_h_to_4h): FalconLinear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (dense_4h_to_h): FalconLinear(in_features=8192, out_features=2048, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): FalconDecoderLayer(\n",
       "        (self_attention): Spy(\n",
       "          (model): FalconAttention(\n",
       "            (query_key_value): FalconLinear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): FalconLinear(in_features=2048, out_features=2048, bias=True)\n",
       "            (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): FalconMLP(\n",
       "          (dense_h_to_4h): FalconLinear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (dense_4h_to_h): FalconLinear(in_features=8192, out_features=2048, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4-23): 20 x FalconDecoderLayer(\n",
       "        (self_attention): FalconAttention(\n",
       "          (query_key_value): FalconLinear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): FalconLinear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): FalconMLP(\n",
       "          (dense_h_to_4h): FalconLinear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (dense_4h_to_h): FalconLinear(in_features=8192, out_features=2048, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
